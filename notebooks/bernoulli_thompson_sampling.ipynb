{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from collections import deque\n",
    "from typing import List, Tuple\n",
    "\n",
    "rng = np.random.default_rng(432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Introduction to Multi-Armed Bandits by Aleksandrs Slivkins](https://arxiv.org/abs/1904.07272)\n",
    "### Assumptions\n",
    "\n",
    "* __IID rewards__: the reward for each arm is drawn independently from a fixed distribution that depends on the arm but not on the round t. The rewards do not vary over time.\n",
    "* __bandit feedback__: The algorithm observes only the reward for the selected action, and nothing else. In particular, it does not observe rewards for other actions that could have been selected.\n",
    "\n",
    "* independent priors and posteriors\n",
    "\n",
    "* TODO: add rewards are bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson sampling algorithm (Beta-Bernoulli)\n",
    "\n",
    "Assign the initial parameters $\\alpha$ and $\\beta$ to each arm. \n",
    "\n",
    "\n",
    "For each round $t$ = 1,2,... $T$, do:\n",
    "1. Sample a mean reward for each arm $k$ using its parameters\n",
    "\n",
    "    $\\mu_t(a_k) \\sim  Beta(\\alpha_k, \\beta_k)$ \n",
    "\n",
    "2. Select the arm with the largest mean reward $a_{k^*}$\n",
    "3. Observe the reward $r_t$ of that arm and update its parameters\n",
    "\n",
    "    $(\\alpha_{k^*}, \\beta_{k^*}) \\leftarrow (\\alpha_{k^*} + r_t, \\beta_{k^*} + 1 - r_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a Beta prior distribution to each arm\n",
    "Initialize each arm with a Beta distributon as prior for the mean reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bandists = 5\n",
    "# expected rewards sampled from a Beta(1, 1) dist\n",
    "\n",
    "expected_rewards = rng.beta(1, 1, size=n_bandists)\n",
    "bandits = [stats.bernoulli(p=p) for p in expected_rewards]\n",
    "best_expected_reward = max(expected_rewards)\n",
    "best_arm = np.argmax(expected_rewards)\n",
    "assert expected_rewards[best_arm] == best_expected_reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #bandits, #parameters per bandit: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# total rounds\n",
    "T = 50\n",
    "\n",
    "# num_bandits, (alpha, beta) parameters\n",
    "initial_parameters = np.ones((n_bandists, 2))\n",
    "parameters = [initial_parameters]\n",
    "print(f' #bandits, #parameters per bandit: {initial_parameters.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# refactor loop\n",
    "# plot  25 x 2, titple t round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "H: List[Tuple[int, int]] = []\n",
    "beliefs = []\n",
    "for t in range(0, T):\n",
    "    parameters_t = parameters[t].copy()\n",
    "    samples_p = [stats.beta(p[0], p[1]).rvs() for p in parameters_t]\n",
    "    k = np.argmax(samples_p)\n",
    "    r = bandits[k].rvs()\n",
    "    H.append((k, r))\n",
    "    if r == 1:\n",
    "        parameters_t[k, 0] += 1\n",
    "    else:\n",
    "        parameters_t[k, 1] += 1\n",
    "    parameters.append(parameters_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regret: $R(T) = \\mu^* \\times T - \\sum_t^T\\mu(a_t) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5868815814590391"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regret(expected_rewards, history) -> float:\n",
    "    best_arm_benchmark = max(expected_rewards) * len(history)\n",
    "    cumulative_reward = sum(reward for arm, reward in history)\n",
    "    return best_arm_benchmark - cumulative_reward\n",
    "\n",
    "\n",
    "regret(expected_rewards, H)\n",
    "\n",
    "# TODO: add expected regret, it could be an histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13003388, 0.38059356, 0.21934638, 0.69844892, 0.81173763])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check possible error in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected_rewards = [0.10, 0.20, 0.80, 0.60]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
